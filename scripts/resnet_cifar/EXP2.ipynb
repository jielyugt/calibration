{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate ResNet 50 on CIFAR 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Input, add, Activation, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import optimizers, regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "from os import path\n",
    "sys.path.append(path.dirname(path.dirname(path.abspath(\"utility\"))))\n",
    "from utility.evaluation import evaluate_model\n",
    "import glob\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solves out of memory issue\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_n            = 8           \n",
    "num_classes        = 10\n",
    "img_rows, img_cols = 32, 32\n",
    "img_channels       = 3\n",
    "batch_size         = 512\n",
    "epochs             = 500\n",
    "iterations         = 45000 // batch_size\n",
    "weight_decay       = 0.0001\n",
    "seed = 333\n",
    "\n",
    "label_smoothing_r  = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch):\n",
    "    if epoch < 250:\n",
    "        return 0.1\n",
    "    if epoch < 375:\n",
    "        return 0.01\n",
    "    return 0.001\n",
    "\n",
    "def residual_network(img_input,classes_num=10,stack_n=8):\n",
    "    def residual_block(intput,out_channel,increase=False):\n",
    "        if increase:\n",
    "            stride = (2,2)\n",
    "        else:\n",
    "            stride = (1,1)\n",
    "\n",
    "        pre_bn   = BatchNormalization()(intput)\n",
    "        pre_relu = Activation('relu')(pre_bn)\n",
    "\n",
    "        conv_1 = Conv2D(out_channel,kernel_size=(3,3),strides=stride,padding='same',\n",
    "                        kernel_initializer=\"he_normal\",\n",
    "                        kernel_regularizer=regularizers.l2(weight_decay))(pre_relu)\n",
    "        bn_1   = BatchNormalization()(conv_1)\n",
    "        relu1  = Activation('relu')(bn_1)\n",
    "        conv_2 = Conv2D(out_channel,kernel_size=(3,3),strides=(1,1),padding='same',\n",
    "                        kernel_initializer=\"he_normal\",\n",
    "                        kernel_regularizer=regularizers.l2(weight_decay))(relu1)\n",
    "        if increase:\n",
    "            projection = Conv2D(out_channel,\n",
    "                                kernel_size=(1,1),\n",
    "                                strides=(2,2),\n",
    "                                padding='same',\n",
    "                                kernel_initializer=\"he_normal\",\n",
    "                                kernel_regularizer=regularizers.l2(weight_decay))(intput)\n",
    "            block = add([conv_2, projection])\n",
    "        else:\n",
    "            block = add([intput,conv_2])\n",
    "        return block\n",
    "    \n",
    "    # input: 32x32x3 output: 32x32x16\n",
    "    x = Conv2D(filters=16,kernel_size=(3,3),strides=(1,1),padding='same',\n",
    "               kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=regularizers.l2(weight_decay))(img_input)\n",
    "\n",
    "    # input: 32x32x16 output: 32x32x16\n",
    "    for _ in range(stack_n):\n",
    "        x = residual_block(x,16,False)\n",
    "\n",
    "    # input: 32x32x16 output: 16x16x32\n",
    "    x = residual_block(x,32,True)\n",
    "    for _ in range(1,stack_n):\n",
    "        x = residual_block(x,32,False)\n",
    "    \n",
    "    # input: 16x16x32 output: 8x8x64\n",
    "    x = residual_block(x,64,True)\n",
    "    for _ in range(1,stack_n):\n",
    "        x = residual_block(x,64,False)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # input: 64 output: 10\n",
    "    x = Dense(classes_num,activation='softmax',\n",
    "              kernel_initializer=\"he_normal\",\n",
    "              kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train_50, y_train_50), (x_test_10, y_test_10) = cifar10.load_data()\n",
    "y_train_50 = keras.utils.to_categorical(y_train_50, num_classes)\n",
    "y_test_10 = keras.utils.to_categorical(y_test_10, num_classes)\n",
    "\n",
    "x_train_45, x_val_5, y_train_45, y_val_5 = train_test_split(x_train_50, y_train_50, test_size=0.1, random_state=seed)\n",
    "\n",
    "img_mean = x_train_45.mean(axis=0)\n",
    "img_std = x_train_45.std(axis=0)\n",
    "x_train_45 = (x_train_45-img_mean)/img_std\n",
    "x_val_5 = (x_val_5-img_mean)/img_std\n",
    "x_test_10 = (x_test_10-img_mean)/img_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build network\n",
    "img_input = Input(shape=(img_rows,img_cols,img_channels))\n",
    "output    = residual_network(img_input,num_classes,stack_n)\n",
    "resnet    = Model(img_input, output)\n",
    "# print(resnet.summary())\n",
    "\n",
    "# set optimizer\n",
    "optimizer = optimizers.SGD(lr=.1, momentum=0.9, nesterov=True)\n",
    "metrics = ['accuracy', 'categorical_crossentropy']\n",
    "\n",
    "if label_smoothing_r > 0:\n",
    "    loss = CategoricalCrossentropy(label_smoothing=label_smoothing_r)\n",
    "else:\n",
    "    loss = CategoricalCrossentropy()\n",
    "resnet.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "# set checkpoint\n",
    "checkpoint_dir = '../../models/EXP2/ls_{}/'.format(str(int(label_smoothing_r*10)))\n",
    "checkpoint_path = checkpoint_dir + 'weights.{epoch:03d}.hdf5'\n",
    "\n",
    "print('writing checkpoints to ' + checkpoint_dir)\n",
    "\n",
    "!rm -r $checkpoint_dir\n",
    "!mkdir $checkpoint_dir\n",
    "\n",
    "cp_callback = ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, verbose=1, period=5)\n",
    "\n",
    "# set callback\n",
    "cbks = [LearningRateScheduler(scheduler), cp_callback]\n",
    "\n",
    "# set data augmentation\n",
    "print('Using real-time data augmentation.')\n",
    "datagen = ImageDataGenerator(horizontal_flip=True,\n",
    "                             width_shift_range=0.125,\n",
    "                             height_shift_range=0.125,\n",
    "                             fill_mode='constant',cval=0.)\n",
    "\n",
    "datagen.fit(x_train_45)\n",
    "\n",
    "# start training\n",
    "hist = resnet.fit_generator(datagen.flow(x_train_45, y_train_45, batch_size=batch_size),\n",
    "                     steps_per_epoch=iterations,\n",
    "                     epochs=epochs,\n",
    "                     callbacks=cbks,\n",
    "                     validation_data=(x_val_5, y_val_5))\n",
    "\n",
    "# print(\"Get test accuracy:\")\n",
    "# loss, accuracy = resnet.evaluate(x_test, y_test, verbose=0)\n",
    "# print(\"Test: accuracy1 = %f  ;  loss1 = %f\" % (accuracy, loss))\n",
    "\n",
    "# save history\n",
    "pickle_path = \"../../logits/EXP2/hist_{}.pkl\".format(str(int(label_smoothing_r*10)))\n",
    "pickle_out = open(pickle_path,\"wb\")\n",
    "pickle.dump(hist.history, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_rate = [0, 0.1]\n",
    "history_list = {}\n",
    "for rate in ls_rate:    \n",
    "    pickle_path = \"../../logits/EXP2/hist_{}.pkl\".format(str(int(rate*10)))\n",
    "    pickle_in = open(pickle_path,\"rb\")\n",
    "    hist = pickle.load(pickle_in)\n",
    "    history_list[rate] = hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Test Error & NLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_r = 0.1\n",
    "hist = history_list[ls_r]\n",
    "\n",
    "error = [(1 - i)*100 for i in hist['val_accuracy']]\n",
    "nll = [i*20 + 5 for i in hist['val_categorical_crossentropy']]\n",
    "plt.axis([0, len(error), 5, 30])\n",
    "plt.plot(error)\n",
    "plt.plot(nll)\n",
    "plt.title('Test Error & NLL (label smoothing rate = {})'.format(ls_r))\n",
    "plt.ylabel('Error (%)/NLL (Scaled)')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Test Error', 'Test NLL'])\n",
    "plt.savefig('../../logits/EXP2/plots/error_nll_{}.png'.format(str(int(ls_r*10))), dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Test Error at different Label Smoothing Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in history_list:\n",
    "    error = [(1 -j)*100 for j in history_list[i]['val_accuracy']]\n",
    "    plt.plot(error)\n",
    "\n",
    "plt.axis([0, len(error), 5, 30])\n",
    "plt.title('Test Error at different Label Smoothing Rate')\n",
    "plt.ylabel('Error (%)')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['no label smoothing', 'label smoothing rate=0.1'])\n",
    "plt.savefig('../../logits/EXP2/plots/error.png', dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot NLL at different Label Smoothing Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in history_list:\n",
    "    nll = [j for j in history_list[i]['val_categorical_crossentropy']]\n",
    "    plt.plot(nll)\n",
    "\n",
    "plt.axis([0, len(error), 0.2, 1.5])\n",
    "plt.title('Test NLL at different Label Smoothing Rate')\n",
    "plt.ylabel('NLL')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['no label smoothing', 'label smoothing rate=0.1'])\n",
    "plt.savefig('../../logits/EXP2/plots/nll.png', dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ECE, MCE and Reliability Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "eva_lr_rate = 0.1\n",
    "eva_lr_rate_str = str(int(eva_lr_rate*10))\n",
    "print('evaluating model with label smoothing value of ' + str(eva_lr_rate))\n",
    "cps = [5,10,30,50,70,100,150,200,300,400,500]\n",
    "def evaluate_epoch(cp):\n",
    "    cp_path = glob.glob('../../models/EXP2/ls_{}/weights.{:03d}.hdf5'.format(eva_lr_rate_str, cp))\n",
    "    if len(cp_path) != 1:\n",
    "        print(cp_path)\n",
    "        raise Exception('checkpoint name confusion')\n",
    "    cp_path = cp_path[0]\n",
    "    cp_name = cp_path.split('/')[-1]\n",
    "\n",
    "    img_input = Input(shape=(img_rows,img_cols,img_channels))\n",
    "    output = residual_network(img_input,num_classes,stack_n)\n",
    "    resnet_random = Model(img_input, output)\n",
    "\n",
    "    accuracy, ece, mce = evaluate_model(resnet_random, cp_path, x_test_10, y_test_10, bins = 15, verbose = True, \n",
    "                   pickle_file = '../../logits/EXP2/hist_{}/cp_'.format(eva_lr_rate_str) + str(cp), x_val = x_val_5, y_val = y_val_5)\n",
    "    return accuracy, ece, mce\n",
    "\n",
    "csv_path = '../../logits/EXP2/hist_{}/results.csv'.format(eva_lr_rate_str)\n",
    "\n",
    "with open(csv_path, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['Epoch', 'Accuracy', 'ECE', 'MCE'])\n",
    "\n",
    "    for cp in cps:\n",
    "        print('[{} Epochs]\\n'.format(cp))\n",
    "        accuracy, ece, mce = evaluate_epoch(cp)\n",
    "        csv_writer.writerow([str(cp), str(accuracy), str(ece), str(mce)])\n",
    "        print('\\n---------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
