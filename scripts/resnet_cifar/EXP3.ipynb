{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate ResNet 20 on CIFAR 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from tensorflow.keras.layers import AveragePooling2D, Input, Flatten\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "# from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from os import path\n",
    "sys.path.append(path.dirname(path.dirname(path.abspath(\"utility\"))))\n",
    "from utility.evaluation import softmax, ECE, MCE\n",
    "import sklearn.metrics as metrics\n",
    "import glob\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solves out of memory issue\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "epochs = 500\n",
    "data_augmentation = True\n",
    "num_classes = 10\n",
    "subtract_pixel_mean = True\n",
    "n = 3\n",
    "depth = n * 6 + 2\n",
    "seed = 86"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "if subtract_pixel_mean:\n",
    "    x_train_mean = np.mean(x_train, axis=0)\n",
    "    x_train -= x_train_mean\n",
    "    x_test -= x_train_mean\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=seed)\n",
    "input_shape = x_train.shape[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model from https://keras.io/examples/cifar10_resnet/\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    return lr\n",
    "\n",
    "def resnet_layer(inputs,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
    "\n",
    "    # Arguments\n",
    "        inputs (tensor): input tensor from input image or previous layer\n",
    "        num_filters (int): Conv2D number of filters\n",
    "        kernel_size (int): Conv2D square kernel dimensions\n",
    "        strides (int): Conv2D square stride dimensions\n",
    "        activation (string): activation name\n",
    "        batch_normalization (bool): whether to include batch normalization\n",
    "        conv_first (bool): conv-bn-activation (True) or\n",
    "            bn-activation-conv (False)\n",
    "\n",
    "    # Returns\n",
    "        x (tensor): tensor as input to the next layer\n",
    "    \"\"\"\n",
    "    conv = Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "        x = conv(x)\n",
    "    return x\n",
    "\n",
    "def resnet(input_shape, depth, num_classes=10):\n",
    "    \"\"\"ResNet Version 1 Model builder\n",
    "    \n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 6 != 0:\n",
    "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
    "    num_filters = 16\n",
    "    num_res_blocks = int((depth - 2) / 6)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = resnet_layer(inputs=inputs)\n",
    "    for stack in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            strides = 1\n",
    "            if stack > 0 and res_block == 0:\n",
    "                strides = 2  # downsample\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters,\n",
    "                             strides=strides)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters,\n",
    "                             activation=None)\n",
    "            if stack > 0 and res_block == 0:\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = keras.layers.add([x, y])\n",
    "            x = Activation('relu')(x)\n",
    "        num_filters *= 2\n",
    "\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten()(x)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# defind optimizer\n",
    "optimizer = Adam(learning_rate=lr_schedule(0))\n",
    "metric = ['accuracy', 'categorical_crossentropy']\n",
    "loss = CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet(input_shape=input_shape, depth=depth)\n",
    "model.summary()\n",
    "\n",
    "# set optimizer\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metric)\n",
    "\n",
    "# set checkpoint path\n",
    "checkpoint_dir = '../../models/EXP3/'\n",
    "checkpoint_path = checkpoint_dir + 'weights.{epoch:03d}.hdf5'\n",
    "!rm -r $checkpoint_dir\n",
    "!mkdir $checkpoint_dir\n",
    "\n",
    "# set callbacks\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "checkpoint = ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, verbose=1, period=10)\n",
    "callbacks = [lr_scheduler, checkpoint]\n",
    "\n",
    "# lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "#                                cooldown=0,\n",
    "#                                patience=5,\n",
    "#                                min_lr=0.5e-6)\n",
    "\n",
    "# callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
    "\n",
    "# set data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    zca_epsilon=1e-06,\n",
    "    rotation_range=0,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.,\n",
    "    zoom_range=0.,\n",
    "    channel_shift_range=0.,\n",
    "    fill_mode='nearest',\n",
    "    cval=0.,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,\n",
    "    rescale=None,\n",
    "    preprocessing_function=None,\n",
    "    data_format=None,\n",
    "    validation_split=0.0)\n",
    "\n",
    "datagen.fit(x_train)\n",
    "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    epochs=epochs,\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "# save history\n",
    "pickle_path = \"../../logits/EXP3/hist.pkl\"\n",
    "pickle_out = open(pickle_path,\"wb\")\n",
    "pickle.dump(hist.history, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_path = \"../../logits/EXP3/hist.pkl\"\n",
    "pickle_in = open(pickle_path,\"rb\")\n",
    "hist = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Test Error & NLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = [(1 - i)*100 for i in hist['val_accuracy']]\n",
    "nll = [i*20 + 5 for i in hist['val_categorical_crossentropy']]\n",
    "plt.axis([0, len(error), 5, 25])\n",
    "plt.plot(error)\n",
    "plt.plot(nll)\n",
    "plt.title('Test Error & NLL')\n",
    "plt.ylabel('Error (%)/NLL (Scaled)')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Test Error', 'Test NLL'])\n",
    "plt.savefig('../../logits/EXP3/plots/error_nll.png', dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, weights_file, x_test, y_test, x_val, y_val, bins = 15, pickle_file = None):\n",
    "    \"\"\"Evaluates the model, calculates the calibration errors and saves the logits\n",
    "    \n",
    "    Args:\n",
    "        model (keras.model): constructed model\n",
    "        weights (string): path to weights file\n",
    "        x_test: (numpy.ndarray) with test data\n",
    "        y_test: (numpy.ndarray) with test data labels\n",
    "        verbose: (boolean) print out results or just return these\n",
    "        pickle_file: (string) path to pickle probabilities given by model\n",
    "        \n",
    "    Returns:\n",
    "        (acc, ece, mce): accuracy of model, ECE and MCE (calibration errors)\n",
    "    \"\"\"    \n",
    "    # Change last activation to linear (instead of softmax)\n",
    "    last_layer = model.layers.pop()\n",
    "    last_layer.activation = keras.activations.linear\n",
    "    i = model.input\n",
    "    o = last_layer(model.layers[-2].output)\n",
    "    model = keras.models.Model(inputs=i, outputs=[o])\n",
    "    \n",
    "    # First load in the weights\n",
    "    model.load_weights(weights_file)\n",
    "    model.compile(optimizer=optimizer, loss=loss)\n",
    "    \n",
    "    # If 1-hot representation, get back to numeric \n",
    "    if y_test.shape[1] > 1: \n",
    "        y_test = np.array([[np.where(r==1)[0][0]] for r in y_test])\n",
    "    \n",
    "    if y_val.shape[1] > 1: \n",
    "        y_val = np.array([[np.where(r==1)[0][0]] for r in y_val])\n",
    "\n",
    "\n",
    "    # Next get predictions\n",
    "    y_logits_test = model.predict(x_test, verbose=1)\n",
    "    y_probs_test = softmax(y_logits_test)\n",
    "    y_preds_test = np.argmax(y_probs_test, axis=1)\n",
    "    \n",
    "    # Confidence of prediction\n",
    "    y_confs_test = np.max(y_probs_test, axis=1)  # Take only maximum confidence\n",
    "    # Calculate Accuracy\n",
    "    accuracy = metrics.accuracy_score(y_test, y_preds_test) * 100\n",
    "    # Calculate ECE\n",
    "    ece = ECE(y_confs_test, y_preds_test, y_val, bin_size = 1/bins)\n",
    "    # Calculate MCE\n",
    "    mce = MCE(y_confs_test, y_preds_test, y_val, bin_size = 1/bins)\n",
    "    \n",
    "    y_logits_val = model.predict(x_val)\n",
    "    y_probs_val = softmax(y_logits_val)\n",
    "    y_preds_val = np.argmax(y_probs_val, axis=1)\n",
    "        \n",
    "    # Pickle probabilities for test and validation\n",
    "    if pickle_file:\n",
    "        with open(pickle_file + '_logits.p', 'wb') as f:\n",
    "            pickle.dump([(y_logits_val, y_val),(y_logits_test, y_test)], f)\n",
    "    \n",
    "    # Return the basic results\n",
    "    return (accuracy, ece, mce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cps = [10,30,50,70,100,150,200,300,400,500]\n",
    "def evaluate_epoch(cp):\n",
    "    cp_path = glob.glob('../../models/EXP3/weights.{:03d}.hdf5'.format(cp))\n",
    "    if len(cp_path) != 1:\n",
    "        print(cp_path)\n",
    "        raise Exception('checkpoint name confusion')\n",
    "    cp_path = cp_path[0]\n",
    "    cp_name = cp_path.split('/')[-1]\n",
    "\n",
    "    model = resnet(input_shape=input_shape, depth=depth)\n",
    "    # model.summary()\n",
    "\n",
    "    accuracy, ece, mce = evaluate_model(model, cp_path, x_test, y_test, x_val, y_val, bins = 15, \n",
    "                   pickle_file = '../../logits/EXP3/cp_' + str(cp))\n",
    "    return accuracy, ece, mce\n",
    "\n",
    "csv_path = '../../logits/EXP3/results.csv'\n",
    "\n",
    "with open(csv_path, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['Epoch', 'Accuracy', 'ECE', 'MCE'])\n",
    "\n",
    "    for cp in cps:\n",
    "        print('[{} Epochs]\\n'.format(cp))\n",
    "        accuracy, ece, mce = evaluate_epoch(cp)\n",
    "        csv_writer.writerow([str(cp), str(accuracy), str(ece), str(mce)])\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        print(\"ECE:\", ece)\n",
    "        print(\"MCE:\", mce)\n",
    "        print('\\n---------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
